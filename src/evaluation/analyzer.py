"""
Evaluation Analyzer
===================

P3-3: è©•ä¾¡ãƒ»åˆ†æ
- P3-3-1: ãƒã‚§ãƒƒã‚¯çµæœãƒ¬ãƒ“ãƒ¥ãƒ¼
- P3-3-2: æ˜¯æ­£ææ¡ˆå¦¥å½“æ€§è©•ä¾¡
- P3-3-3: False Positive/Negativeåˆ†æ
- P3-3-4: RAGæ–¹å¼åˆ¥ç²¾åº¦æ¯”è¼ƒåˆ†æ
- P3-3-5: æ”¹å–„ç‚¹ç‰¹å®šãƒ»å„ªå…ˆåº¦ä»˜ã‘
"""

from datetime import datetime, UTC
from typing import Optional
from collections import defaultdict

from pydantic import BaseModel, Field

from src.evaluation.models import (
    EvaluationResult,
    EvaluationSummary,
    ErrorAnalysis,
    RAGComparison,
    ImprovementSuggestion,
    DocumentEvaluationResult,
    CheckEvaluationResult,
)


class AnalysisReport(BaseModel):
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆ"""
    report_id: str
    created_at: str = Field(
        default_factory=lambda: datetime.now(UTC).isoformat()
    )
    
    # ç·åˆè©•ä¾¡
    overall_accuracy: float = 0.0
    overall_precision: float = 0.0
    overall_recall: float = 0.0
    overall_f1_score: float = 0.0
    
    # False Positive/Negativeåˆ†æ
    error_analysis: list[ErrorAnalysis] = Field(default_factory=list)
    
    # RAGæ–¹å¼æ¯”è¼ƒ
    rag_comparisons: list[RAGComparison] = Field(default_factory=list)
    
    # æ”¹å–„ææ¡ˆ
    improvement_suggestions: list[ImprovementSuggestion] = Field(default_factory=list)
    
    # è©³ç´°åˆ†æ
    check_item_analysis: dict[str, dict] = Field(default_factory=dict)
    
    # å†ç¾æ€§åˆ†æ
    reproducibility_rate: float = 1.0
    reproducibility_notes: list[str] = Field(default_factory=list)


class EvaluationAnalyzer:
    """è©•ä¾¡çµæœã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self):
        self.results: list[EvaluationResult] = []
    
    def add_result(self, result: EvaluationResult):
        """è©•ä¾¡çµæœã‚’è¿½åŠ """
        self.results.append(result)
    
    def analyze(self) -> AnalysisReport:
        """ç·åˆåˆ†æã‚’å®Ÿè¡Œ"""
        if not self.results:
            raise ValueError("No evaluation results to analyze")
        
        report_id = f"analysis-{datetime.now(UTC).strftime('%Y%m%d%H%M%S')}"
        report = AnalysisReport(report_id=report_id)
        
        # ç·åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        self._calculate_overall_metrics(report)
        
        # ã‚¨ãƒ©ãƒ¼åˆ†æ
        self._analyze_errors(report)
        
        # ãƒã‚§ãƒƒã‚¯é …ç›®åˆ¥åˆ†æ
        self._analyze_by_check_item(report)
        
        # å†ç¾æ€§åˆ†æ
        self._analyze_reproducibility(report)
        
        # æ”¹å–„ææ¡ˆç”Ÿæˆ
        self._generate_improvements(report)
        
        return report
    
    def _calculate_overall_metrics(self, report: AnalysisReport):
        """ç·åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        total_tp = sum(r.summary.true_positives for r in self.results)
        total_tn = sum(r.summary.true_negatives for r in self.results)
        total_fp = sum(r.summary.false_positives for r in self.results)
        total_fn = sum(r.summary.false_negatives for r in self.results)
        total_all = total_tp + total_tn + total_fp + total_fn
        
        if total_all > 0:
            report.overall_accuracy = (total_tp + total_tn) / total_all
        
        if total_tp + total_fp > 0:
            report.overall_precision = total_tp / (total_tp + total_fp)
        
        if total_tp + total_fn > 0:
            report.overall_recall = total_tp / (total_tp + total_fn)
        
        if report.overall_precision + report.overall_recall > 0:
            report.overall_f1_score = (
                2 * report.overall_precision * report.overall_recall /
                (report.overall_precision + report.overall_recall)
            )
    
    def _analyze_errors(self, report: AnalysisReport):
        """False Positive/Negativeåˆ†æ"""
        # ãƒã‚§ãƒƒã‚¯é …ç›®ã”ã¨ã®ã‚¨ãƒ©ãƒ¼é›†è¨ˆ
        error_counts: dict[str, dict] = defaultdict(lambda: {
            "fp_count": 0,
            "fn_count": 0,
            "fp_examples": [],
            "fn_examples": [],
        })
        
        for result in self.results:
            for doc_result in result.document_results:
                for check_result in doc_result.check_results:
                    if not check_result.is_correct:
                        check_id = check_result.check_item_id
                        
                        # False Positive: å®Ÿéš›ã¯passãªã®ã«failã¨åˆ¤å®š
                        if check_result.expected_result == "pass" and check_result.actual_result == "fail":
                            error_counts[check_id]["fp_count"] += 1
                            error_counts[check_id]["fp_examples"].append(doc_result.document_id)
                        
                        # False Negative: å®Ÿéš›ã¯failãªã®ã«passã¨åˆ¤å®š
                        elif check_result.expected_result == "fail" and check_result.actual_result == "pass":
                            error_counts[check_id]["fn_count"] += 1
                            error_counts[check_id]["fn_examples"].append(doc_result.document_id)
        
        # ErrorAnalysisã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        for check_id, counts in error_counts.items():
            if counts["fp_count"] > 0 or counts["fn_count"] > 0:
                report.error_analysis.append(ErrorAnalysis(
                    check_item_id=check_id,
                    check_item_name=check_id,
                    false_positive_count=counts["fp_count"],
                    false_negative_count=counts["fn_count"],
                    false_positive_examples=counts["fp_examples"][:5],
                    false_negative_examples=counts["fn_examples"][:5],
                ))
    
    def _analyze_by_check_item(self, report: AnalysisReport):
        """ãƒã‚§ãƒƒã‚¯é …ç›®åˆ¥åˆ†æ"""
        check_stats: dict[str, dict] = defaultdict(lambda: {
            "total": 0,
            "correct": 0,
            "tp": 0,
            "tn": 0,
            "fp": 0,
            "fn": 0,
        })
        
        for result in self.results:
            for doc_result in result.document_results:
                for check_result in doc_result.check_results:
                    check_id = check_result.check_item_id
                    stats = check_stats[check_id]
                    stats["total"] += 1
                    
                    if check_result.is_correct:
                        stats["correct"] += 1
                        if check_result.expected_result == "fail":
                            stats["tp"] += 1
                        else:
                            stats["tn"] += 1
                    else:
                        if check_result.expected_result == "pass":
                            stats["fp"] += 1
                        else:
                            stats["fn"] += 1
        
        # ãƒã‚§ãƒƒã‚¯é …ç›®åˆ¥ãƒ¬ãƒãƒ¼ãƒˆ
        for check_id, stats in check_stats.items():
            accuracy = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
            precision = stats["tp"] / (stats["tp"] + stats["fp"]) if stats["tp"] + stats["fp"] > 0 else 0
            recall = stats["tp"] / (stats["tp"] + stats["fn"]) if stats["tp"] + stats["fn"] > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0
            
            report.check_item_analysis[check_id] = {
                "total_evaluations": stats["total"],
                "correct": stats["correct"],
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1_score": f1,
                "true_positives": stats["tp"],
                "true_negatives": stats["tn"],
                "false_positives": stats["fp"],
                "false_negatives": stats["fn"],
            }
    
    def _analyze_reproducibility(self, report: AnalysisReport):
        """å†ç¾æ€§åˆ†æ"""
        consistent_runs = 0
        total_runs = 0
        
        for result in self.results:
            if result.repeat_results:
                hashes = [r.results_hash for r in result.repeat_results]
                if len(set(hashes)) == 1:
                    consistent_runs += len(hashes)
                else:
                    # ç•°ãªã‚‹çµæœãŒã‚ã‚‹
                    unique_count = len(set(hashes))
                    report.reproducibility_notes.append(
                        f"{result.config.name}: {unique_count}ç¨®é¡ã®ç•°ãªã‚‹çµæœ"
                    )
                total_runs += len(hashes)
        
        if total_runs > 0:
            report.reproducibility_rate = consistent_runs / total_runs
        
        if report.reproducibility_rate == 1.0:
            report.reproducibility_notes.append("å…¨ã¦ã®ç¹°ã‚Šè¿”ã—å®Ÿè¡Œã§ä¸€è²«ã—ãŸçµæœã‚’ç¢ºèª")
    
    def _generate_improvements(self, report: AnalysisReport):
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        # ã‚¨ãƒ©ãƒ¼åˆ†æã«åŸºã¥ãæ”¹å–„ææ¡ˆ
        for error in report.error_analysis:
            if error.false_positive_count > 0:
                report.improvement_suggestions.append(ImprovementSuggestion(
                    priority="high" if error.false_positive_count >= 3 else "medium",
                    category="check_logic",
                    description=f"{error.check_item_id}: False Positiveå‰Šæ¸› ({error.false_positive_count}ä»¶)",
                    expected_impact="Precisionå‘ä¸Š",
                    effort_estimate="1-2æ—¥",
                ))
            
            if error.false_negative_count > 0:
                report.improvement_suggestions.append(ImprovementSuggestion(
                    priority="high" if error.false_negative_count >= 3 else "medium",
                    category="check_logic",
                    description=f"{error.check_item_id}: False Negativeå‰Šæ¸› ({error.false_negative_count}ä»¶)",
                    expected_impact="Recallå‘ä¸Š",
                    effort_estimate="1-2æ—¥",
                ))
        
        # å…¨ä½“ç²¾åº¦ã«åŸºã¥ãæ”¹å–„ææ¡ˆ
        if report.overall_accuracy < 0.8:
            report.improvement_suggestions.append(ImprovementSuggestion(
                priority="high",
                category="prompt",
                description="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š",
                expected_impact="Accuracy 80%ä»¥ä¸Šé”æˆ",
                effort_estimate="2-3æ—¥",
            ))
        
        if report.overall_precision < 0.7:
            report.improvement_suggestions.append(ImprovementSuggestion(
                priority="high",
                category="check_logic",
                description="åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®å³å¯†åŒ–",
                expected_impact="Precisionå‘ä¸Š",
                effort_estimate="2-3æ—¥",
            ))
        
        if report.overall_recall < 0.7:
            report.improvement_suggestions.append(ImprovementSuggestion(
                priority="high",
                category="check_logic",
                description="åˆ¤å®šæ¡ä»¶ã®è¦‹ç›´ã—",
                expected_impact="Recallå‘ä¸Š",
                effort_estimate="2-3æ—¥",
            ))
        
        # RAGæ”¹å–„ææ¡ˆ
        report.improvement_suggestions.append(ImprovementSuggestion(
            priority="medium",
            category="rag",
            description="LLMçµ±åˆã«ã‚ˆã‚‹é«˜ç²¾åº¦ãƒã‚§ãƒƒã‚¯",
            expected_impact="å…¨ä½“ç²¾åº¦10-20%å‘ä¸Š",
            effort_estimate="1é€±é–“",
        ))
        
        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        priority_order = {"high": 0, "medium": 1, "low": 2}
        report.improvement_suggestions.sort(
            key=lambda x: priority_order.get(x.priority, 99)
        )


def create_analysis_report(
    results: list[EvaluationResult],
) -> AnalysisReport:
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    analyzer = EvaluationAnalyzer()
    for result in results:
        analyzer.add_result(result)
    return analyzer.analyze()


def format_analysis_report(report: AnalysisReport) -> str:
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    lines = []
    
    lines.append("=" * 60)
    lines.append("ğŸ“Š SmartReviewer PoC è©•ä¾¡åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("=" * 60)
    lines.append(f"ãƒ¬ãƒãƒ¼ãƒˆID: {report.report_id}")
    lines.append(f"ä½œæˆæ—¥æ™‚: {report.created_at}")
    
    lines.append("\n" + "=" * 60)
    lines.append("ğŸ“ˆ ç·åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹")
    lines.append("=" * 60)
    lines.append(f"Accuracy:  {report.overall_accuracy:.1%}")
    lines.append(f"Precision: {report.overall_precision:.1%}")
    lines.append(f"Recall:    {report.overall_recall:.1%}")
    lines.append(f"F1 Score:  {report.overall_f1_score:.1%}")
    
    if report.error_analysis:
        lines.append("\n" + "=" * 60)
        lines.append("ğŸ” False Positive/Negativeåˆ†æ")
        lines.append("=" * 60)
        for error in report.error_analysis:
            lines.append(f"\n{error.check_item_id}:")
            if error.false_positive_count > 0:
                lines.append(f"  - False Positive: {error.false_positive_count}ä»¶")
            if error.false_negative_count > 0:
                lines.append(f"  - False Negative: {error.false_negative_count}ä»¶")
    
    if report.check_item_analysis:
        lines.append("\n" + "=" * 60)
        lines.append("ğŸ“‹ ãƒã‚§ãƒƒã‚¯é …ç›®åˆ¥åˆ†æ")
        lines.append("=" * 60)
        for check_id, analysis in report.check_item_analysis.items():
            lines.append(f"\n{check_id}:")
            lines.append(f"  - Accuracy: {analysis['accuracy']:.1%}")
            lines.append(f"  - Precision: {analysis['precision']:.1%}")
            lines.append(f"  - Recall: {analysis['recall']:.1%}")
    
    lines.append("\n" + "=" * 60)
    lines.append("ğŸ”„ å†ç¾æ€§åˆ†æ")
    lines.append("=" * 60)
    lines.append(f"å†ç¾æ€§ç‡: {report.reproducibility_rate:.1%}")
    for note in report.reproducibility_notes:
        lines.append(f"  - {note}")
    
    if report.improvement_suggestions:
        lines.append("\n" + "=" * 60)
        lines.append("ğŸ’¡ æ”¹å–„ææ¡ˆ")
        lines.append("=" * 60)
        for i, suggestion in enumerate(report.improvement_suggestions, 1):
            priority_icon = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(suggestion.priority, "âšª")
            lines.append(f"\n{i}. [{priority_icon} {suggestion.priority.upper()}] {suggestion.description}")
            lines.append(f"   ã‚«ãƒ†ã‚´ãƒª: {suggestion.category}")
            lines.append(f"   æœŸå¾…åŠ¹æœ: {suggestion.expected_impact}")
            lines.append(f"   å·¥æ•°: {suggestion.effort_estimate}")
    
    return "\n".join(lines)
