#!/usr/bin/env python
"""
PoCå®Ÿæ–½ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
================

P3-2: PoCå®Ÿæ–½
- P3-2-1: åŸºæœ¬è¨­è¨ˆæ›¸ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œï¼ˆ10ä»¶ï¼‰
- P3-2-2: ãƒ†ã‚¹ãƒˆè¨ˆç”»æ›¸ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œï¼ˆ10ä»¶ï¼‰
- P3-2-3: ãƒã‚§ãƒƒã‚¯åˆè‡´ç‡è¨ˆæ¸¬
- P3-2-4: å‡¦ç†æ™‚é–“ãƒ»ã‚³ã‚¹ãƒˆè¨ˆæ¸¬
- P3-2-5: å†ç¾æ€§æ¤œè¨¼ï¼ˆåŒä¸€å…¥åŠ›è¤‡æ•°å›å®Ÿè¡Œï¼‰
"""

import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.evaluation import (
    EvaluationRunner,
    EvaluationConfig,
    EvaluationResult,
    EvaluationStatus,
    create_basic_design_dataset,
    create_test_plan_dataset,
    run_evaluation_streaming,
)


def create_poc_config(
    name: str,
    dataset_id: str,
    repeat_count: int = 3,
) -> EvaluationConfig:
    """PoCç”¨è©•ä¾¡è¨­å®šã‚’ä½œæˆ"""
    return EvaluationConfig(
        name=name,
        dataset_id=dataset_id,
        repeat_count=repeat_count,
        timeout_seconds=300,
        use_llm=False,  # PoC Phase 1ã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿
    )


async def run_basic_design_evaluation(
    runner: EvaluationRunner,
    repeat_count: int = 3,
) -> EvaluationResult:
    """åŸºæœ¬è¨­è¨ˆæ›¸ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
    dataset = create_basic_design_dataset()
    runner.register_dataset(dataset)
    
    config = create_poc_config(
        name="PoC-åŸºæœ¬è¨­è¨ˆæ›¸è©•ä¾¡",
        dataset_id=dataset.id,
        repeat_count=repeat_count,
    )
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ åŸºæœ¬è¨­è¨ˆæ›¸ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
    print(f"   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset.name}")
    print(f"   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(dataset.documents)}")
    print(f"   - ç¹°ã‚Šè¿”ã—å›æ•°: {repeat_count}")
    print(f"{'='*60}")
    
    result = await runner.run_evaluation(config)
    return result


async def run_test_plan_evaluation(
    runner: EvaluationRunner,
    repeat_count: int = 3,
) -> EvaluationResult:
    """ãƒ†ã‚¹ãƒˆè¨ˆç”»æ›¸ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
    dataset = create_test_plan_dataset()
    runner.register_dataset(dataset)
    
    config = create_poc_config(
        name="PoC-ãƒ†ã‚¹ãƒˆè¨ˆç”»æ›¸è©•ä¾¡",
        dataset_id=dataset.id,
        repeat_count=repeat_count,
    )
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆè¨ˆç”»æ›¸ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
    print(f"   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset.name}")
    print(f"   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(dataset.documents)}")
    print(f"   - ç¹°ã‚Šè¿”ã—å›æ•°: {repeat_count}")
    print(f"{'='*60}")
    
    result = await runner.run_evaluation(config)
    return result


def print_result_summary(result: EvaluationResult, title: str):
    """è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    summary = result.summary
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {title} - çµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.status.value}")
    print(f"å‡¦ç†æ™‚é–“: {summary.total_processing_time_ms / 1000:.2f}ç§’")
    print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {summary.total_documents}")
    print(f"ãƒã‚§ãƒƒã‚¯é …ç›®æ•°: {summary.total_checks}")
    
    print(f"\nğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
    print(f"   - Accuracy: {summary.accuracy:.1%}")
    print(f"   - Precision: {summary.precision:.1%}")
    print(f"   - Recall: {summary.recall:.1%}")
    print(f"   - F1 Score: {summary.f1_score:.1%}")
    
    print(f"\nğŸ“Š æ··åŒè¡Œåˆ—:")
    print(f"   - True Positive (TP): {summary.true_positives}")
    print(f"   - True Negative (TN): {summary.true_negatives}")
    print(f"   - False Positive (FP): {summary.false_positives}")
    print(f"   - False Negative (FN): {summary.false_negatives}")
    
    if result.repeat_results:
        print(f"\nğŸ”„ å†ç¾æ€§æ¤œè¨¼:")
        print(f"   - ç¹°ã‚Šè¿”ã—å›æ•°: {len(result.repeat_results)}")
        
        # çµæœã®ãƒãƒƒã‚·ãƒ¥ã§ä¸€è²«æ€§ã‚’ç¢ºèª
        hashes = [r.results_hash for r in result.repeat_results]
        unique_hashes = set(hashes)
        consistency_count = sum(1 for h in hashes if hashes.count(h) == len(hashes))
        
        if len(unique_hashes) == 1:
            print(f"   - çµæœä¸€è²«æ€§: 100% (å…¨å®Ÿè¡Œã§åŒä¸€çµæœ)")
        else:
            print(f"   - çµæœä¸€è²«æ€§: {len(unique_hashes)}ç¨®é¡ã®ç•°ãªã‚‹çµæœ")
        
        # å„å®Ÿè¡Œã®Accuracy
        avg_accuracy = sum(r.accuracy for r in result.repeat_results) / len(result.repeat_results)
        print(f"   - å¹³å‡Accuracy: {avg_accuracy:.1%}")


def print_detailed_results(result: EvaluationResult):
    """è©³ç´°çµæœã‚’è¡¨ç¤º"""
    print(f"\n{'='*60}")
    print(f"ğŸ“ è©³ç´°çµæœ")
    print(f"{'='*60}")
    
    for doc_result in result.document_results:
        print(f"\nğŸ“„ {doc_result.document_id}")
        print(f"   å‡¦ç†æ™‚é–“: {doc_result.processing_time:.2f}ç§’")
        
        for check_result in doc_result.check_results:
            status_icon = "âœ…" if check_result.is_correct else "âŒ"
            print(f"   {status_icon} {check_result.check_id}")
            print(f"      äºˆæ¸¬: {check_result.predicted} / æ­£è§£: {check_result.expected}")


async def run_poc_evaluation(
    repeat_count: int = 3,
    output_dir: Optional[Path] = None,
    verbose: bool = False,
):
    """PoCè©•ä¾¡ã‚’å®Ÿè¡Œ"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ SmartReviewer PoCè©•ä¾¡é–‹å§‹")
    print(f"   å®Ÿè¡Œæ—¥æ™‚: {datetime.now().isoformat()}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    # ãƒ©ãƒ³ãƒŠãƒ¼ä½œæˆ
    runner = EvaluationRunner(use_llm=False)
    
    # åŸºæœ¬è¨­è¨ˆæ›¸ãƒã‚§ãƒƒã‚¯
    bd_result = await run_basic_design_evaluation(runner, repeat_count)
    print_result_summary(bd_result, "åŸºæœ¬è¨­è¨ˆæ›¸")
    if verbose:
        print_detailed_results(bd_result)
    
    # ãƒ†ã‚¹ãƒˆè¨ˆç”»æ›¸ãƒã‚§ãƒƒã‚¯
    tp_result = await run_test_plan_evaluation(runner, repeat_count)
    print_result_summary(tp_result, "ãƒ†ã‚¹ãƒˆè¨ˆç”»æ›¸")
    if verbose:
        print_detailed_results(tp_result)
    
    # ç·åˆçµæœ
    total_time = time.time() - start_time
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š PoCè©•ä¾¡ ç·åˆçµæœ")
    print(f"{'='*60}")
    print(f"ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
    
    # ç·åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    total_tp = bd_result.summary.true_positives + tp_result.summary.true_positives
    total_tn = bd_result.summary.true_negatives + tp_result.summary.true_negatives
    total_fp = bd_result.summary.false_positives + tp_result.summary.false_positives
    total_fn = bd_result.summary.false_negatives + tp_result.summary.false_negatives
    total_all = total_tp + total_tn + total_fp + total_fn
    
    if total_all > 0:
        accuracy = (total_tp + total_tn) / total_all
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"\nğŸ“ˆ ç·åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        print(f"   - Accuracy: {accuracy:.1%}")
        print(f"   - Precision: {precision:.1%}")
        print(f"   - Recall: {recall:.1%}")
        print(f"   - F1 Score: {f1:.1%}")
    
    # çµæœä¿å­˜
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åŸºæœ¬è¨­è¨ˆæ›¸çµæœ
        bd_file = output_dir / f"poc_basic_design_{timestamp}.json"
        with open(bd_file, "w", encoding="utf-8") as f:
            json.dump(bd_result.model_dump(mode="json"), f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ åŸºæœ¬è¨­è¨ˆæ›¸çµæœä¿å­˜: {bd_file}")
        
        # ãƒ†ã‚¹ãƒˆè¨ˆç”»æ›¸çµæœ
        tp_file = output_dir / f"poc_test_plan_{timestamp}.json"
        with open(tp_file, "w", encoding="utf-8") as f:
            json.dump(tp_result.model_dump(mode="json"), f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆè¨ˆç”»æ›¸çµæœä¿å­˜: {tp_file}")
        
        # ç·åˆã‚µãƒãƒªãƒ¼
        summary_file = output_dir / f"poc_summary_{timestamp}.json"
        summary_data = {
            "execution_time": datetime.now().isoformat(),
            "total_processing_time": total_time,
            "basic_design": {
                "status": bd_result.status.value,
                "accuracy": bd_result.summary.accuracy,
                "precision": bd_result.summary.precision,
                "recall": bd_result.summary.recall,
                "f1_score": bd_result.summary.f1_score,
            },
            "test_plan": {
                "status": tp_result.status.value,
                "accuracy": tp_result.summary.accuracy,
                "precision": tp_result.summary.precision,
                "recall": tp_result.summary.recall,
                "f1_score": tp_result.summary.f1_score,
            },
            "total": {
                "accuracy": accuracy if total_all > 0 else 0,
                "precision": precision if total_all > 0 else 0,
                "recall": recall if total_all > 0 else 0,
                "f1_score": f1 if total_all > 0 else 0,
            },
        }
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ç·åˆã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_file}")
    
    print(f"\n{'='*60}")
    print(f"âœ… PoCè©•ä¾¡å®Œäº†")
    print(f"{'='*60}")
    
    return bd_result, tp_result


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="SmartReviewer PoCè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument(
        "--repeat",
        type=int,
        default=3,
        help="å†ç¾æ€§æ¤œè¨¼ã®ç¹°ã‚Šè¿”ã—å›æ•° (default: 3)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="storage/poc_results",
        help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: storage/poc_results)",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="è©³ç´°çµæœã‚’è¡¨ç¤º",
    )
    
    args = parser.parse_args()
    
    asyncio.run(run_poc_evaluation(
        repeat_count=args.repeat,
        output_dir=Path(args.output) if args.output else None,
        verbose=args.verbose,
    ))


if __name__ == "__main__":
    main()
